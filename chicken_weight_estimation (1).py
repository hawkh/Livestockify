# -*- coding: utf-8 -*-
"""Chicken weight estimation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17ti7FIP-4QD4Pbyq_7rBG3VyJ983awcu
"""

!pip install ultralytics

import ultralytics
ultralytics.checks()

!unzip /content/labelleddata.zip -d /content/

!unzip /content/dataset.zip -d /content/

# Load a COCO-pretrained YOLOv5n model and train it on the COCO8 example dataset for 100 epochs
!yolo train model=yolov5n.pt data=/content/labelleddata/data.yaml epochs=100 imgsz=640

!yolo task=detect mode=train model=yolov5x6u data=/content/labelleddata/data.yaml \
  epochs=100 imgsz=1280 batch=2 augment=True lr0=0.01 lrf=0.1 momentum=0.937 weight_decay=0.0005 \
  warmup_epochs=3 warmup_momentum=0.8 warmup_bias_lr=0.1 flipud=0.1 fliplr=0.5 mosaic=1.0 mixup=0.3 \
  degrees=20 translate=0.3 scale=0.5 shear=5 perspective=0.001 hsv_h=0.015 hsv_s=0.8 hsv_v=0.6 \
  copy_paste=0.2 multi_scale=True plots=True

!yolo task=detect mode=train model=runs/progressive/phase1/weights/best.pt \
  data=/content/labelleddata/data.yaml \
  epochs=75 imgsz=1024 batch=4 \
  lr0=0.005 lrf=0.01 optimizer=AdamW cos_lr=True \
  degrees=15 translate=0.2 scale=0.7 shear=5 \
  mosaic=0.7 mixup=0.1 copy_paste=0.2 \
  hsv_h=0.015 hsv_s=0.6 hsv_v=0.4 \
  erasing=0.2 label_smoothing=0.05 \
  project=runs/progressive name=phase2

!yolo task=detect mode=train model=runs/progressive/phase2/weights/best.pt \
  data=/content/labelleddata/data.yaml \
  epochs=100 imgsz=1280 batch=2 \
  lr0=0.002 lrf=0.01 optimizer=AdamW cos_lr=True \
  degrees=20 translate=0.3 scale=0.9 shear=8 \
  mosaic=0.8 mixup=0.2 copy_paste=0.3 \
  hsv_h=0.02 hsv_s=0.7 hsv_v=0.5 \
  erasing=0.4 label_smoothing=0.1 \
  multi_scale=True warmup_epochs=5 \
  project=runs/progressive name=phase3_final

!yolo task=detect mode=train model=yolo11m.pt data=/content/labelleddata/data.yaml \
  epochs=150 imgsz=1280 batch=2 \
  degrees=25 translate=0.4 scale=1.2 shear=10 \
  mosaic=1.0 mixup=0.3 copy_paste=0.4 \
  erasing=0.5 label_smoothing=0.15 \
  optimizer=AdamW lr0=0.008 \
  project=runs/ensemble name=aggressive

!yolo task=detect mode=train model=yolo11m.pt data=/content/labelleddata/data.yaml \
  epochs=150 imgsz=1280 batch=2 \
  degrees=15 translate=0.2 scale=0.8 \
  hsv_h=0.03 hsv_s=0.9 hsv_v=0.7 \
  mosaic=0.7 mixup=0.15 copy_paste=0.25 \
  optimizer=AdamW lr0=0.008 \
  project=runs/ensemble name=color_focused

!yolo task=detect mode=train model=yolo11m.pt data=/content/labelleddata/data.yaml \
  epochs=120 imgsz=1280 batch=2 \
  optimizer=NAdam lr0=0.01 lrf=0.005 \
  momentum=0.9 weight_decay=0.001 \
  cos_lr=True warmup_epochs=5 warmup_bias_lr=0.05 \
  project=runs/optimizer_test name=nadam

!yolo task=detect mode=train model=yolo11m.pt data=/content/labelleddata/data.yaml \
  epochs=150 imgsz=1280 batch=2 \
  multi_scale=True rect=False \
  degrees=18 translate=0.25 scale=0.8 shear=6 \
  mosaic=0.85 mixup=0.2 copy_paste=0.35 \
  hsv_h=0.018 hsv_s=0.75 hsv_v=0.55 \
  erasing=0.35 label_smoothing=0.08 \
  optimizer=AdamW lr0=0.01 lrf=0.008 cos_lr=True \
  patience=30 save_period=15 \
  project=runs/multiscale name=dynamic_input

!yolo task=detect mode=train model=yolo11m.pt data=/content/labelleddata/data.yaml \
  epochs=130 imgsz=1280 batch=2 \
  # Simulate multiple scales and flips
  multi_scale=True \
  degrees=0 translate=0.05 scale=0.95 shear=0 \
  fliplr=0.8 flipud=0.2 \
  mosaic=0.3 mixup=0.0 copy_paste=0.1 \
  hsv_h=0.005 hsv_s=0.3 hsv_v=0.2 \
  optimizer=AdamW lr0=0.006 \
  project=runs/tta_simulation name=tta_train

!yolo task=detect mode=train model=yolo11m.pt data=/content/labelleddata/data.yaml \
  epochs=140 imgsz=1280 batch=2 \
  erasing=0.6 \
  degrees=20 translate=0.3 scale=0.9 \
  mosaic=0.8 mixup=0.25 copy_paste=0.3 \
  project=runs/advanced_aug name=gridmask_sim

!yolo task=detect mode=train model=yolo11m.pt data=/content/labelleddata/data.yaml \
  epochs=140 imgsz=1280 batch=2 \
  copy_paste=0.6 mosaic=0.4 mixup=0.0 \
  degrees=15 translate=0.2 scale=0.8 \
  erasing=0.2 label_smoothing=0.1 \
  project=runs/advanced_aug name=cutmix_sim

#validation strategies
!yolo task=detect mode=val model=runs/progressive/phase3_final/weights/best.pt \
  data=/content/labelleddata/data.yaml \
  imgsz=640 save_txt=True save_conf=True project=runs/val name=res640

!yolo task=detect mode=val model=runs/progressive/phase3_final/weights/best.pt \
  data=/content/labelleddata/data.yaml \
  imgsz=1280 save_txt=True save_conf=True project=runs/val name=res1280

!yolo task=detect mode=val model=runs/progressive/phase3_final/weights/best.pt \
  data=/content/labelleddata/data.yaml \
  imgsz=1280 augment=True save_txt=True project=runs/val name=tta_val

from ultralytics import YOLO

# Load a COCO-pretrained YOLO11n model
model = YOLO("yolo11n.pt")

results = model.train(data="/content/data.yaml", epochs=50, imgsz=640)

!yolo task=detect mode=predict model=/content/runs/detect/train4/weights/best.pt conf=0.95 source=/content/labelleddata/train/images save=True

import cv2

from ultralytics import YOLO

# Load the YOLO model
model = YOLO("/content/runs/detect/train2/weights/best.pt")

# Open the video file
video_path = "/content/chickvideo.mp4"
cap = cv2.VideoCapture(video_path)

# Loop through the video frames
while cap.isOpened():
    # Read a frame from the video
    success, frame = cap.read()

    if success:
        # Run YOLO inference on the frame
        results = model(frame)

        # Visualize the results on the frame
        annotated_frame = results[0].plot()

        # Display the annotated frame
        cv2.imshow("YOLO Inference", annotated_frame)

        # Break the loop if 'q' is pressed
        if cv2.waitKey(1) & 0xFF == ord("q"):
            break
    else:
        # Break the loop if the end of the video is reached
        break

# Release the video capture object and close the display window
cap.release()
cv2.destroyAllWindows()

import cv2
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from ultralytics import YOLO
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import joblib
from typing import Tuple, List, Dict
import math

class ChickenWeightEstimator:
    """
    Advanced chicken weight estimation using multiple deep learning approaches
    """

    def __init__(self, yolo_model_path: str):
        self.yolo_model = YOLO(yolo_model_path)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.weight_model = None
        self.scaler = StandardScaler()

    def extract_features(self, image: np.ndarray, bbox: List[float]) -> np.ndarray:
        """
        Extract comprehensive features from chicken bounding box
        """
        x1, y1, x2, y2 = map(int, bbox)

        # Crop the chicken region
        chicken_crop = image[y1:y2, x1:x2]

        if chicken_crop.size == 0:
            return np.zeros(20)  # Return default features if crop is empty

        # Basic dimensional features
        width = x2 - x1
        height = y2 - y1
        area = width * height
        aspect_ratio = width / height if height > 0 else 1.0

        # Color features
        if len(chicken_crop.shape) == 3:
            mean_color = np.mean(chicken_crop, axis=(0, 1))
            std_color = np.std(chicken_crop, axis=(0, 1))
        else:
            mean_color = np.array([np.mean(chicken_crop)] * 3)
            std_color = np.array([np.std(chicken_crop)] * 3)

        # Texture features using gradients
        gray_crop = cv2.cvtColor(chicken_crop, cv2.COLOR_BGR2GRAY) if len(chicken_crop.shape) == 3 else chicken_crop

        # Sobel gradients
        grad_x = cv2.Sobel(gray_crop, cv2.CV_64F, 1, 0, ksize=3)
        grad_y = cv2.Sobel(gray_crop, cv2.CV_64F, 0, 1, ksize=3)

        gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)
        texture_energy = np.mean(gradient_magnitude)
        texture_variance = np.var(gradient_magnitude)

        # Shape features
        perimeter = 2 * (width + height)  # Approximation
        compactness = (4 * math.pi * area) / (perimeter**2) if perimeter > 0 else 0

        # Combine all features
        features = np.array([
            width, height, area, aspect_ratio, perimeter, compactness,
            texture_energy, texture_variance,
            *mean_color, *std_color
        ])

        # Ensure we have exactly 20 features
        if len(features) < 20:
            features = np.pad(features, (0, 20 - len(features)), 'constant')
        elif len(features) > 20:
            features = features[:20]

        return features

class ChickenWeightDataset(Dataset):
    """
    Dataset class for training weight estimation model
    """

    def __init__(self, images: List[np.ndarray], bboxes: List[List[float]],
                 weights: List[float], transform=None):
        self.images = images
        self.bboxes = bboxes
        self.weights = weights
        self.transform = transform

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image = self.images[idx]
        bbox = self.bboxes[idx]
        weight = self.weights[idx]

        # Extract features
        estimator = ChickenWeightEstimator("")
        features = estimator.extract_features(image, bbox)

        return torch.FloatTensor(features), torch.FloatTensor([weight])

class WeightRegressionModel(nn.Module):
    """
    Neural network for weight regression
    """

    def __init__(self, input_size=20):
        super(WeightRegressionModel, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_size, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.ReLU()  # Ensure positive weights
        )

    def forward(self, x):
        return self.network(x)

class AdvancedChickenWeightEstimator(ChickenWeightEstimator):
    """
    Advanced weight estimation with multiple models and ensemble
    """

    def __init__(self, yolo_model_path: str):
        super().__init__(yolo_model_path)
        self.models = {
            'neural_network': None,
            'morphological': None,
            'statistical': None
        }
        self.ensemble_weights = [0.5, 0.3, 0.2]  # Weights for ensemble

    def train_neural_network(self, training_data: Dict, epochs=100):
        """
        Train neural network for weight estimation
        """
        # Prepare data
        features = []
        weights = []

        for data in training_data:
            feature = self.extract_features(data['image'], data['bbox'])
            features.append(feature)
            weights.append(data['weight'])

        features = np.array(features)
        weights = np.array(weights)

        # Normalize features
        features_scaled = self.scaler.fit_transform(features)

        # Create dataset
        dataset = torch.utils.data.TensorDataset(
            torch.FloatTensor(features_scaled),
            torch.FloatTensor(weights.reshape(-1, 1))
        )

        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

        # Initialize model
        model = WeightRegressionModel(input_size=features.shape[1])
        model = model.to(self.device)

        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=0.001)

        # Training loop
        model.train()
        for epoch in range(epochs):
            epoch_loss = 0
            for batch_features, batch_weights in dataloader:
                batch_features = batch_features.to(self.device)
                batch_weights = batch_weights.to(self.device)

                optimizer.zero_grad()
                outputs = model(batch_features)
                loss = criterion(outputs, batch_weights)
                loss.backward()
                optimizer.step()

                epoch_loss += loss.item()

            if epoch % 20 == 0:
                print(f'Epoch {epoch}, Loss: {epoch_loss/len(dataloader):.4f}')

        self.models['neural_network'] = model

    def morphological_weight_estimation(self, image: np.ndarray, bbox: List[float]) -> float:
        """
        Improved morphological weight estimation
        """
        x1, y1, x2, y2 = map(int, bbox)

        # Extract dimensions
        width = x2 - x1
        height = y2 - y1

        # Estimate depth using aspect ratio and body shape analysis
        aspect_ratio = width / height if height > 0 else 1.0

        # More sophisticated depth estimation
        if aspect_ratio > 1.2:  # Side view
            depth = width * 0.4
        elif aspect_ratio < 0.8:  # Front/back view
            depth = height * 0.3
        else:  # Angled view
            depth = min(width, height) * 0.35

        # Convert to real-world dimensions (assuming calibration)
        pixel_to_cm = 0.2  # This should be calibrated for your setup

        width_cm = width * pixel_to_cm
        height_cm = height * pixel_to_cm
        depth_cm = depth * pixel_to_cm

        # Use ellipsoid volume with correction factors
        volume_cm3 = (4/3) * math.pi * (width_cm/2) * (height_cm/2) * (depth_cm/2)

        # Apply breed-specific density (broilers vs layers)
        density_kg_per_cm3 = 0.00095  # Adjusted for typical broiler density

        # Size-based correction factor
        size_factor = 1.0
        if width_cm > 25:  # Large chicken
            size_factor = 1.1
        elif width_cm < 15:  # Small chicken
            size_factor = 0.9

        weight_kg = volume_cm3 * density_kg_per_cm3 * size_factor

        return max(0.5, min(5.0, weight_kg))  # Reasonable bounds

    def statistical_weight_estimation(self, image: np.ndarray, bbox: List[float]) -> float:
        """
        Statistical weight estimation based on empirical relationships
        """
        features = self.extract_features(image, bbox)

        # Extract key features
        width = features[0]
        height = features[1]
        area = features[2]
        aspect_ratio = features[3]

        # Empirical formula based on typical chicken measurements
        # These coefficients should be calibrated with real data
        weight_kg = (
            0.0001 * area +
            0.02 * width +
            0.015 * height +
            0.3 * aspect_ratio +
            0.8  # Base weight
        )

        return max(0.5, min(5.0, weight_kg))

    def predict_weight(self, image: np.ndarray, bbox: List[float]) -> float:
        """
        Ensemble prediction using multiple models
        """
        predictions = []

        # Neural network prediction
        if self.models['neural_network'] is not None:
            features = self.extract_features(image, bbox)
            features_scaled = self.scaler.transform(features.reshape(1, -1))

            with torch.no_grad():
                nn_pred = self.models['neural_network'](
                    torch.FloatTensor(features_scaled).to(self.device)
                ).cpu().numpy()[0, 0]
            predictions.append(nn_pred)
        else:
            predictions.append(self.morphological_weight_estimation(image, bbox))

        # Morphological prediction
        morph_pred = self.morphological_weight_estimation(image, bbox)
        predictions.append(morph_pred)

        # Statistical prediction
        stat_pred = self.statistical_weight_estimation(image, bbox)
        predictions.append(stat_pred)

        # Ensemble prediction
        ensemble_pred = sum(w * p for w, p in zip(self.ensemble_weights, predictions))

        return max(0.5, min(5.0, ensemble_pred))  # Reasonable bounds

def process_image_with_weights(image_path: str, estimator: AdvancedChickenWeightEstimator) -> Tuple[np.ndarray, List[float]]:
    """
    Process image and estimate weights for all detected chickens
    """
    # Load image
    image = cv2.imread(image_path)
    if image is None:
        raise ValueError(f"Could not load image: {image_path}")

    # Detect chickens
    results = estimator.yolo_model(image_path)

    weights = []
    annotated_image = image.copy()

    for idx, box in enumerate(results[0].boxes.data.tolist()):
        x1, y1, x2, y2, conf, cls = box

        # Filter for chicken class with high confidence
        if int(cls) == 0 and conf > 0.5:
            # Estimate weight
            weight = estimator.predict_weight(image, [x1, y1, x2, y2])
            weights.append(weight)

            # Annotate image
            label = f"Chicken {idx + 1}: {weight:.2f}kg"
            cv2.rectangle(annotated_image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)
            cv2.putText(annotated_image, label, (int(x1), int(y1) - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)

    return annotated_image, weights

# Example usage
def main():
    # Initialize the estimator
    estimator = AdvancedChickenWeightEstimator("/content/runs/detect/train2/weights/best.pt")

    # Example training data (you would replace this with real data)
    # training_data = [
    #     {'image': image1, 'bbox': [x1, y1, x2, y2], 'weight': 2.3},
    #     {'image': image2, 'bbox': [x1, y1, x2, y2], 'weight': 1.8},
    #     # ... more training data
    # ]
    # estimator.train_neural_network(training_data)

    # Process image
    image_path = "/content/train/images/frame_0013_jpg.rf.982fa4bbcabbd1544010e4461c4e64d2.jpg"
    annotated_image, weights = process_image_with_weights(image_path, estimator)

    print(f"Detected {len(weights)} chickens")
    print(f"Estimated weights: {weights}")

    # Display results
    cv2.imwrite("/content/weight_estimation_result.jpg", annotated_image)

    return estimator, annotated_image, weights

if __name__ == "__main__":
    estimator, result_image, chicken_weights = main()

import cv2
from ultralytics import YOLO
import time
from google.colab.patches import cv2_imshow
from IPython.display import clear_output

# Load the YOLO model
model = YOLO("/content/runs/detect/train2/weights/best.pt")

# Open the video file
video_path = "/content/chickvideo.mp4"
cap = cv2.VideoCapture(video_path)

# Get video properties
fps = cap.get(cv2.CAP_PROP_FPS)
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

print(f"Video Properties:")
print(f"FPS: {fps}")
print(f"Resolution: {width}x{height}")
print(f"Total frames: {total_frames}")

# Optional: Save output video
save_output = True
if save_output:
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter('/content/output_video.mp4', fourcc, fps, (width, height))

frame_count = 0
start_time = time.time()

# Loop through the video frames
while cap.isOpened():
    # Read a frame from the video
    success, frame = cap.read()

    if success:
        frame_count += 1

        # Run YOLO inference on the frame
        results = model(frame, conf=0.5, iou=0.45)  # Set confidence and IoU thresholds

        # Visualize the results on the frame
        annotated_frame = results[0].plot()

        # Add frame counter and FPS info
        cv2.putText(annotated_frame, f"Frame: {frame_count}/{total_frames}",
                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

        # Calculate and display current FPS
        current_time = time.time()
        processing_fps = frame_count / (current_time - start_time)
        cv2.putText(annotated_frame, f"Processing FPS: {processing_fps:.1f}",
                   (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

        # Display detection count
        detections = len(results[0].boxes) if results[0].boxes is not None else 0
        cv2.putText(annotated_frame, f"Detections: {detections}",
                   (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

        # Save frame to output video if enabled
        if save_output:
            out.write(annotated_frame)

        # Display the annotated frame in Colab
        if frame_count % 30 == 0:  # Show every 30th frame to avoid overwhelming output
            clear_output(wait=True)
            print(f"Frame {frame_count}/{total_frames} - Detections: {detections}")
            cv2_imshow(annotated_frame)

        # For faster processing without display, comment out the above block

        # Print progress every 100 frames
        if frame_count % 100 == 0:
            progress = (frame_count / total_frames) * 100
            print(f"Progress: {progress:.1f}% ({frame_count}/{total_frames} frames)")
    else:
        # Break the loop if the end of the video is reached
        break

# Cleanup
cap.release()
if save_output:
    out.release()
cv2.destroyAllWindows()  # This won't do anything in Colab but kept for compatibility

print(f"\nProcessing complete!")
print(f"Total frames processed: {frame_count}")
print(f"Average processing FPS: {frame_count / (time.time() - start_time):.2f}")

!yolo task=detect mode=predict model=/content/runs/detect/train2/weights/best.pt conf=0.95 source=/content/train/images save=True

!git clone https://github.com/ultralytics/yolov11
!cd yolov5

!python export.py --weights /content/runs/detect/train/weights/best.pt --include onnx

from pathlib import Path
import torch

# Load YOLOv11 model
model = torch.hub.load('path_to_yolov11_repo', 'custom', path='/content/best.pt', source='local')

# Run inference
results = model('/content/frame_0334.jpg')

# Save YOLO-format annotations
results.save_txt(save_dir='autolabels')  # saves .txt files in YOLO format

!pip install onnxruntime

!pip install onnx onnx-tf

import torch
import torchvision
import onnx
import os

# Load your trained YOLOv11 model
model = torch.load("/content/phase3model.pt", map_location=torch.device('cuda'),weights_only=True)['model'].float()
model.eval()

# Dummy input for export
dummy_input = torch.randn(1, 3, 640, 640)  # adjust if you used different input size

# Export to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "best.onnx",
    opset_version=12,
    input_names=['images'],
    output_names=['output'],
    dynamic_axes={'images': {0: 'batch'}, 'output': {0: 'batch'}}
)

print("✅ Exported to best.onnx")

!git clone https://github.com/ultralytics/yolov5.git
!cd yolov5

!cd yolov5

!python /content/yolov5/export.py --weights /content/runs/detect/train2/weights/best.pt --include tfjs --imgsz 640

# Export a YOLO11n PyTorch model to TF.js format
!yolo export model=/content/runs/detect/train2/weights/best.pt format=tfjs # creates '/yolo11n_web_model'

# Run inference with the exported model

# Export a YOLO11n PyTorch model to TF.js format
!yolo export model=/content/phase3model.pt format=tfjs # creates '/yolo11n_web_model'

import torch
from ultralytics.nn.tasks import DetectionModel  # must exist in your env
torch.serialization.add_safe_globals({'DetectionModel': DetectionModel})

model_data = torch.load('/content/runs/detect/train/weights/best.pt', weights_only=False)
model = model_data['model'].float()
model.eval()

import torch

# Force full load if you trust this file (you trained it yourself)
model_data = torch.load('/content/runs/detect/train/weights/best.pt', weights_only=False)
model = model_data['model'].float()
model.eval()

dummy_input = torch.randn(1, 3, 640, 640)

torch.onnx.export(
    model,
    dummy_input,
    "best.onnx",
    opset_version=12,
    input_names=['images'],
    output_names=['output'],
    dynamic_axes={'images': {0: 'batch'}, 'output': {0: 'batch'}}
)

print("✅ Exported to best.onnx")

!onnx-tf convert -i /content/best.onnx -o tf_model

!pip uninstall -y tensorflow tensorflow-addons keras

!pip install tensorflow==2.15.0
!pip install tensorflow-addons==0.23.0
!pip install keras==2.15.0

!onnx-tf convert -i /content/best.onnx -o tf_model

!pip uninstall -y tensorflow tensorflow-addons tensorflow-probability keras onnx-tf

!pip install tensorflow==2.15.0
!pip install keras==2.15.0
!pip install tensorflow-addons==0.23.0
!pip install onnx==1.15.0
!pip install onnx-tf==1.10.0

!pip install onnx-tf==1.9.0  # NOT 1.10.0, to avoid TFP

!onnx-tf convert -i /content/best.onnx -o tf_model

!pip install tensorflowjs

!tensorflowjs_converter \
  --input_format=tf_saved_model \
  --output_format=tfjs_graph_model \
  tf_model \
  tfjs_model

!zip -r /content/tfjs_model.zip /content/tfjs_model

import pandas as pd

# Load JSON file
df = pd.read_json("/content/2004.json")

# Show DataFrame
print(df)

import json
from pandas import json_normalize

# Read JSON file
with open("/content/2004.json") as f:
    data = json.load(f)

# Flatten and convert to DataFrame
df = json_normalize(data)

print(df)

import pandas as pd

# Load your JSON file
df = pd.read_json("/content/2004.json")

# Remove rows where any value is exactly 0
df = df[(df != 0).all(axis=1)]

# Save to CSV
df.to_csv("outputwihout0.csv", index=False)

# prompt: remove ammonia attribute

# Read JSON file
with open("/content/2004.json") as f:
    data = json.load(f)

# Flatten and convert to DataFrame
df = json_normalize(data)

# Remove the 'ammonia' column
if 'ammonia' in df.columns:
    df = df.drop(columns=['deviceReading.ammonia'])

# Print the modified DataFrame (optional)
df

# You can save this modified DataFrame to a new file if needed
# df.to_csv("output_without_ammonia.csv", index=False)
# df.to_json("output_without_ammonia.json", orient='records', indent=2)

from matplotlib import pyplot as plt
df['deviceReading.humidity'].plot(kind='line', figsize=(8, 4), title='deviceReading.humidity')
plt.gca().spines[['top', 'right']].set_visible(False)

import json
import pandas as pd
from pandas import json_normalize

# Load nested JSON
with open("/content/2004.json") as f:
    data = json.load(f)

# Flatten the JSON structure
df = json_normalize(data)

# Save to CSV
df.to_csv("output.csv", index=False)

import cv2
import numpy as np
import torch
import torch.nn as nn
from ultralytics import YOLO
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import joblib
from typing import Tuple, List, Dict, Optional
import math
from dataclasses import dataclass
from enum import Enum
import json

class ChickenAgeCategory(Enum):
    """Chicken age categories with typical weight ranges"""
    DAY_OLD = (0.035, 0.045)  # 35-45 grams
    WEEK_1 = (0.150, 0.200)   # 150-200 grams
    WEEK_2 = (0.400, 0.500)   # 400-500 grams
    WEEK_3 = (0.800, 1.000)   # 800g-1kg
    WEEK_4 = (1.200, 1.500)   # 1.2-1.5kg
    WEEK_5 = (1.800, 2.200)   # 1.8-2.2kg
    WEEK_6 = (2.500, 3.000)   # 2.5-3kg
    ADULT = (3.000, 5.000)    # 3-5kg

@dataclass
class CameraCalibration:
    """Camera calibration parameters"""
    focal_length: float = 1000.0  # in pixels
    sensor_width: float = 6.0     # in mm
    sensor_height: float = 4.5    # in mm
    known_object_width: float = 25.0  # average adult chicken width in cm

class AdvancedChickenWeightEstimator:
    """
    Enhanced chicken weight estimation with distance compensation and age classification
    """

    def __init__(self, yolo_model_path: str, camera_calibration: Optional[CameraCalibration] = None):
        self.yolo_model = YOLO(yolo_model_path)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.camera_calib = camera_calibration or CameraCalibration()
        self.scaler = StandardScaler()
        self.age_classifier = self._build_age_classifier()

    def _build_age_classifier(self):
        """Build a simple neural network for age classification"""
        model = nn.Sequential(
            nn.Linear(10, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, len(ChickenAgeCategory))
        )
        return model.to(self.device)

    def estimate_distance(self, bbox: List[float], image_shape: Tuple[int, int]) -> float:
        """
        Estimate distance from camera to chicken using perspective projection
        """
        x1, y1, x2, y2 = bbox
        bbox_width_pixels = x2 - x1
        image_width = image_shape[1]

        # Calculate field of view
        fov = 2 * math.atan(self.camera_calib.sensor_width / (2 * self.camera_calib.focal_length))

        # Estimate real width based on distance
        # Using the pinhole camera model: distance = (real_width * focal_length) / pixel_width
        estimated_distance = (self.camera_calib.known_object_width * self.camera_calib.focal_length) / bbox_width_pixels

        return estimated_distance

    def classify_chicken_age(self, features: np.ndarray) -> ChickenAgeCategory:
        """
        Classify chicken age based on visual features
        """
        # Simplified age classification based on size features
        area = features[2]  # Assuming area is at index 2

        # Rule-based classification (can be replaced with trained model)
        if area < 1000:
            return ChickenAgeCategory.DAY_OLD
        elif area < 2500:
            return ChickenAgeCategory.WEEK_1
        elif area < 5000:
            return ChickenAgeCategory.WEEK_2
        elif area < 8000:
            return ChickenAgeCategory.WEEK_3
        elif area < 12000:
            return ChickenAgeCategory.WEEK_4
        elif area < 18000:
            return ChickenAgeCategory.WEEK_5
        elif area < 25000:
            return ChickenAgeCategory.WEEK_6
        else:
            return ChickenAgeCategory.ADULT

    def extract_enhanced_features(self, image: np.ndarray, bbox: List[float]) -> np.ndarray:
        """
        Extract comprehensive features including distance-aware measurements
        """
        x1, y1, x2, y2 = map(int, bbox)

        # Estimate distance to compensate for perspective
        distance = self.estimate_distance(bbox, image.shape[:2])

        # Crop the chicken region
        chicken_crop = image[y1:y2, x1:x2]

        if chicken_crop.size == 0:
            return np.zeros(30)  # Increased feature size

        # Basic dimensional features
        width = x2 - x1
        height = y2 - y1
        area = width * height
        aspect_ratio = width / height if height > 0 else 1.0

        # Distance-compensated measurements
        distance_factor = distance / 100.0  # Normalize to meters
        compensated_width = width * distance_factor
        compensated_height = height * distance_factor
        compensated_area = compensated_width * compensated_height

        # Color features
        if len(chicken_crop.shape) == 3:
            # Convert to multiple color spaces for better feature extraction
            hsv_crop = cv2.cvtColor(chicken_crop, cv2.COLOR_BGR2HSV)
            lab_crop = cv2.cvtColor(chicken_crop, cv2.COLOR_BGR2LAB)

            # RGB statistics
            mean_bgr = np.mean(chicken_crop, axis=(0, 1))
            std_bgr = np.std(chicken_crop, axis=(0, 1))

            # HSV statistics (useful for white/yellow chicken detection)
            mean_hsv = np.mean(hsv_crop, axis=(0, 1))
            std_hsv = np.std(hsv_crop, axis=(0, 1))

            # LAB statistics
            mean_lab = np.mean(lab_crop, axis=(0, 1))
        else:
            mean_bgr = np.array([np.mean(chicken_crop)] * 3)
            std_bgr = np.array([np.std(chicken_crop)] * 3)
            mean_hsv = mean_bgr
            std_hsv = std_bgr
            mean_lab = mean_bgr

        # Enhanced texture features
        gray_crop = cv2.cvtColor(chicken_crop, cv2.COLOR_BGR2GRAY) if len(chicken_crop.shape) == 3 else chicken_crop

        # Gabor filters for texture analysis
        texture_features = self._extract_texture_features(gray_crop)

        # Shape features
        perimeter = 2 * (width + height)
        compactness = (4 * math.pi * area) / (perimeter**2) if perimeter > 0 else 0

        # Edge density (feather texture indicator)
        edges = cv2.Canny(gray_crop, 50, 150)
        edge_density = np.sum(edges > 0) / (width * height) if area > 0 else 0

        # Combine all features
        features = np.concatenate([
            [width, height, area, aspect_ratio, perimeter, compactness],
            [compensated_width, compensated_height, compensated_area],
            [distance_factor, edge_density],
            mean_bgr, std_bgr,
            mean_hsv[:2], std_hsv[:2],  # Only H and S channels
            texture_features[:5]  # Top 5 texture features
        ])

        # Ensure we have exactly 30 features
        if len(features) < 30:
            features = np.pad(features, (0, 30 - len(features)), 'constant')
        elif len(features) > 30:
            features = features[:30]

        return features

    def _extract_texture_features(self, gray_image: np.ndarray) -> np.ndarray:
        """Extract texture features using various filters"""
        features = []

        # Sobel gradients
        grad_x = cv2.Sobel(gray_image, cv2.CV_64F, 1, 0, ksize=3)
        grad_y = cv2.Sobel(gray_image, cv2.CV_64F, 0, 1, ksize=3)
        gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)

        features.extend([
            np.mean(gradient_magnitude),
            np.std(gradient_magnitude),
            np.max(gradient_magnitude)
        ])

        # Laplacian (detect edges)
        laplacian = cv2.Laplacian(gray_image, cv2.CV_64F)
        features.extend([
            np.mean(np.abs(laplacian)),
            np.std(laplacian)
        ])

        return np.array(features)

    def estimate_weight_with_age_constraint(self, features: np.ndarray, bbox: List[float], image: np.ndarray) -> float:
        """
        Estimate weight with age category constraints
        """
        # Classify age category
        age_category = self.classify_chicken_age(features)

        # Get weight range for the age category
        min_weight, max_weight = age_category.value

        # Distance-compensated measurements
        distance = self.estimate_distance(bbox, image.shape[:2])

        # Extract compensated dimensions
        width = features[6]  # compensated_width
        height = features[7]  # compensated_height
        area = features[8]   # compensated_area

        # Age-specific weight estimation
        if age_category == ChickenAgeCategory.DAY_OLD:
            # Day-old chicks: very light, fluffy
            base_weight = 0.040  # 40 grams
            weight_kg = base_weight + (area / 1000000) * 0.005

        elif age_category in [ChickenAgeCategory.WEEK_1, ChickenAgeCategory.WEEK_2]:
            # Young chicks: rapid growth phase
            base_weight = min_weight
            growth_factor = (area / 10000) * 0.15
            weight_kg = base_weight + growth_factor

        elif age_category in [ChickenAgeCategory.WEEK_3, ChickenAgeCategory.WEEK_4]:
            # Juvenile: moderate growth
            volume_estimate = (width * height * (width * 0.7)) / 1000000  # Approximate volume in cubic meters
            density = 950  # kg/m³ (slightly less than water due to feathers)
            weight_kg = volume_estimate * density

        else:
            # Adult or near-adult: use full morphological estimation
            # Ellipsoid model with breed-specific adjustments
            depth_estimate = min(width, height) * 0.8  # More accurate depth estimation
            volume_cm3 = (4/3) * math.pi * (width/2) * (height/2) * (depth_estimate/2)

            # Adjust density based on size (larger chickens are denser)
            density_kg_per_cm3 = 0.00095 + (area / 1000000) * 0.00005
            weight_kg = volume_cm3 * density_kg_per_cm3

        # Constrain to age-appropriate range
        weight_kg = max(min_weight, min(max_weight, weight_kg))

        return weight_kg

    def process_video_with_weights(self, video_path: str, output_path: str,
                                 display_frame_interval: int = 30,
                                 save_annotations: bool = True):
        """
        Process video and add weight estimations to each frame
        """
        cap = cv2.VideoCapture(video_path)

        # Get video properties
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        # Setup video writer
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

        # Initialize weight prediction models
        self.weight_regression_model = WeightRegressionModel(input_size=30).to(self.device)

        frame_count = 0
        weight_history = {}  # Track weights across frames for smoothing

        print(f"Processing video: {video_path}")
        print(f"Total frames: {total_frames}")

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            frame_count += 1

            # Run YOLO detection
            results = self.yolo_model(frame, conf=0.5, iou=0.45)

            # Process each detection
            annotated_frame = frame.copy()
            current_weights = []

            if results[0].boxes is not None:
                for idx, box in enumerate(results[0].boxes.data.tolist()):
                    x1, y1, x2, y2, conf, cls = box

                    # Only process chicken detections
                    if int(cls) == 0 and conf > 0.5:
                        bbox = [x1, y1, x2, y2]

                        # Extract enhanced features
                        features = self.extract_enhanced_features(frame, bbox)

                        # Get weight estimation using ensemble method
                        weight_kg = self.ensemble_weight_prediction(frame, bbox, features)

                        # Smooth weight across frames
                        chicken_id = f"{idx}_{int(x1)}_{int(y1)}"
                        if chicken_id in weight_history:
                            # Apply temporal smoothing
                            smoothed_weight = 0.7 * weight_kg + 0.3 * weight_history[chicken_id][-1]
                            weight_history[chicken_id].append(smoothed_weight)
                            # Keep only last 10 frames
                            if len(weight_history[chicken_id]) > 10:
                                weight_history[chicken_id].pop(0)
                            weight_kg = smoothed_weight
                        else:
                            weight_history[chicken_id] = [weight_kg]

                        current_weights.append(weight_kg)

                        # Annotate frame
                        self._annotate_frame(annotated_frame, bbox, weight_kg, idx, conf)

            # Add frame info
            self._add_frame_info(annotated_frame, frame_count, total_frames, len(current_weights))

            # Write frame
            out.write(annotated_frame)

            # Display progress
            if frame_count % display_frame_interval == 0:
                progress = (frame_count / total_frames) * 100
                print(f"Progress: {progress:.1f}% - Frame {frame_count}/{total_frames}")
                if save_annotations:
                    self._save_frame_annotations(frame_count, current_weights)

        # Cleanup
        cap.release()
        out.release()
        cv2.destroyAllWindows()

        print(f"Video processing complete! Output saved to: {output_path}")

        return weight_history

    def ensemble_weight_prediction(self, image: np.ndarray, bbox: List[float], features: np.ndarray) -> float:
        """
        Ensemble prediction using multiple models (from original implementation)
        """
        predictions = []
        weights = []

        # 1. Neural Network Prediction
        if hasattr(self, 'weight_regression_model') and self.weight_regression_model is not None:
            try:
                features_tensor = torch.FloatTensor(features).unsqueeze(0).to(self.device)
                with torch.no_grad():
                    nn_pred = self.weight_regression_model(features_tensor).cpu().numpy()[0, 0]
                predictions.append(nn_pred)
                weights.append(0.4)  # Higher weight for trained model
            except:
                pass

        # 2. Enhanced Morphological Prediction
        morph_pred = self.morphological_weight_estimation_enhanced(image, bbox, features)
        predictions.append(morph_pred)
        weights.append(0.3)

        # 3. Statistical Prediction
        stat_pred = self.statistical_weight_estimation_enhanced(features)
        predictions.append(stat_pred)
        weights.append(0.2)

        # 4. Age-constrained Prediction
        age_pred = self.estimate_weight_with_age_constraint(features, bbox, image)
        predictions.append(age_pred)
        weights.append(0.1)

        # Normalize weights
        weights = np.array(weights) / np.sum(weights)

        # Weighted ensemble
        ensemble_pred = np.sum(np.array(predictions) * weights)

        return ensemble_pred

    def morphological_weight_estimation_enhanced(self, image: np.ndarray, bbox: List[float], features: np.ndarray) -> float:
        """
        Enhanced morphological weight estimation with distance compensation
        """
        x1, y1, x2, y2 = map(int, bbox)

        # Get distance-compensated dimensions from features
        comp_width = features[6]
        comp_height = features[7]
        comp_area = features[8]
        distance_factor = features[9]

        # Estimate depth based on body proportions and viewing angle
        aspect_ratio = comp_width / comp_height if comp_height > 0 else 1.0

        # Improved depth estimation based on chicken anatomy
        if aspect_ratio > 1.3:  # Side view
            depth = comp_width * 0.45
            body_shape_factor = 0.85  # Chickens are more cylindrical from side
        elif aspect_ratio < 0.8:  # Front/back view
            depth = comp_height * 0.35
            body_shape_factor = 0.75  # More compressed from front/back
        else:  # Angled view
            depth = min(comp_width, comp_height) * 0.4
            body_shape_factor = 0.8

        # Calculate volume using modified ellipsoid model
        # Account for chicken body shape (not a perfect ellipsoid)
        volume_cm3 = body_shape_factor * (4/3) * math.pi * (comp_width/2) * (comp_height/2) * (depth/2)

        # Dynamic density based on visual features
        # Younger chickens have lower density due to fluffier feathers
        edge_density = features[10]
        base_density = 0.00095  # kg/cm³

        # Adjust density based on edge density (feather fluffiness indicator)
        if edge_density < 0.1:  # Fluffy, young chicken
            density_kg_per_cm3 = base_density * 0.7
        elif edge_density > 0.3:  # Adult chicken with defined feathers
            density_kg_per_cm3 = base_density * 1.1
        else:
            density_kg_per_cm3 = base_density

        # Calculate weight
        weight_kg = volume_cm3 * density_kg_per_cm3

        # Apply breed-specific corrections if possible
        # White chickens (broilers) tend to be heavier
        mean_bgr = features[11:14]
        if np.mean(mean_bgr) > 200:  # Likely white broiler
            weight_kg *= 1.15

        return weight_kg

    def statistical_weight_estimation_enhanced(self, features: np.ndarray) -> float:
        """
        Enhanced statistical weight estimation using empirical relationships
        """
        # Extract key features
        comp_width = features[6]
        comp_height = features[7]
        comp_area = features[8]
        aspect_ratio = features[3]
        compactness = features[5]
        edge_density = features[10]

        # Multi-factor regression model based on chicken growth curves
        # These coefficients should be calibrated with real data

        # Base weight calculation
        base_weight = 0.5  # 500g base

        # Size-based component
        size_component = (
            0.00008 * comp_area +
            0.015 * comp_width +
            0.012 * comp_height
        )

        # Shape-based component
        shape_component = (
            0.2 * aspect_ratio +
            0.3 * compactness
        )

        # Texture-based component (indicates age/maturity)
        texture_component = edge_density * 2.0

        # Combine components with non-linear growth curve
        weight_kg = base_weight + size_component + shape_component + texture_component

        # Apply sigmoid-like growth curve constraint
        # Chickens grow rapidly initially, then plateau
        growth_factor = 1 - math.exp(-comp_area / 10000)
        weight_kg *= growth_factor

        return weight_kg

    def _annotate_frame(self, frame: np.ndarray, bbox: List[float], weight_kg: float,
                       chicken_id: int, confidence: float):
        """
        Annotate frame with detection box and weight information
        """
        x1, y1, x2, y2 = map(int, bbox)

        # Determine age category for color coding
        features = self.extract_enhanced_features(frame, bbox)
        age_category = self.classify_chicken_age(features)

        # Color based on age
        if age_category == ChickenAgeCategory.DAY_OLD:
            color = (255, 255, 0)  # Cyan for day-old
            age_text = "Day-old"
        elif age_category in [ChickenAgeCategory.WEEK_1, ChickenAgeCategory.WEEK_2]:
            color = (255, 200, 0)  # Light blue
            age_text = f"Week {1 if age_category == ChickenAgeCategory.WEEK_1 else 2}"
        elif age_category in [ChickenAgeCategory.WEEK_3, ChickenAgeCategory.WEEK_4]:
            color = (0, 255, 0)  # Green
            age_text = f"Week {3 if age_category == ChickenAgeCategory.WEEK_3 else 4}"
        else:
            color = (0, 165, 255)  # Orange for adult
            age_text = "Adult"

        # Draw bounding box
        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)

        # Prepare label text
        if weight_kg < 1.0:
            weight_text = f"{weight_kg*1000:.0f}g"
        else:
            weight_text = f"{weight_kg:.2f}kg"

        label = f"#{chicken_id + 1} | {weight_text} | {age_text}"
        conf_text = f"Conf: {confidence:.2f}"

        # Calculate text position and background
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.6
        thickness = 2

        # Get text size
        (label_width, label_height), _ = cv2.getTextSize(label, font, font_scale, thickness)
        (conf_width, conf_height), _ = cv2.getTextSize(conf_text, font, font_scale * 0.8, 1)

        # Draw background rectangles for better text visibility
        cv2.rectangle(frame, (x1, y1 - label_height - 10),
                     (x1 + max(label_width, conf_width) + 10, y1), color, -1)

        # Draw text
        cv2.putText(frame, label, (x1 + 5, y1 - 5), font, font_scale, (255, 255, 255), thickness)
        cv2.putText(frame, conf_text, (x1 + 5, y1 - label_height - 5), font, font_scale * 0.8, (200, 200, 200), 1)

        # Add distance indicator
        distance = self.estimate_distance(bbox, frame.shape[:2])
        dist_text = f"Dist: {distance:.1f}cm"
        cv2.putText(frame, dist_text, (x1, y2 + 20), font, 0.5, color, 1)

    def _add_frame_info(self, frame: np.ndarray, frame_count: int, total_frames: int, detection_count: int):
        """
        Add frame information overlay
        """
        # Create semi-transparent overlay for info box
        overlay = frame.copy()
        info_height = 100
        cv2.rectangle(overlay, (0, 0), (300, info_height), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)

        # Add text info
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.6
        color = (255, 255, 255)

        info_lines = [
            f"Frame: {frame_count}/{total_frames}",
            f"Detections: {detection_count}",
            f"Progress: {(frame_count/total_frames)*100:.1f}%"
        ]

        y_offset = 25
        for i, line in enumerate(info_lines):
            cv2.putText(frame, line, (10, y_offset + i*25), font, font_scale, color, 1)

    def _save_frame_annotations(self, frame_number: int, weights: List[float]):
        """
        Save frame annotations to JSON file for later analysis
        """
        annotations = {
            'frame': frame_number,
            'timestamp': frame_number / 30.0,  # Assuming 30 fps
            'detections': len(weights),
            'weights': weights,
            'average_weight': np.mean(weights) if weights else 0,
            'total_weight': sum(weights)
        }

        # Append to annotations file
        annotations_file = 'chicken_weight_annotations.json'
        try:
            with open(annotations_file, 'r') as f:
                all_annotations = json.load(f)
        except:
            all_annotations = []

        all_annotations.append(annotations)

        with open(annotations_file, 'w') as f:
            json.dump(all_annotations, f, indent=2)

class WeightRegressionModel(nn.Module):
    """
    Enhanced neural network for weight regression with residual connections
    """
    def __init__(self, input_size=30):
        super(WeightRegressionModel, self).__init__()

        # Feature extraction layers
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_size, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.3),

            nn.Linear(64, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.3),

            nn.Linear(128, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.2)
        )

        # Regression head
        self.regression_head = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.ReLU()  # Ensure positive weights
        )

        # Residual connection
        self.residual = nn.Linear(input_size, 1)

    def forward(self, x):
        # Main path
        features = self.feature_extractor(x)
        weight = self.regression_head(features)

        # Residual path for direct weight estimation
        residual = self.residual(x)

        # Combine with residual
        output = weight + 0.1 * torch.relu(residual)

        return output

def train_weight_model(estimator: AdvancedChickenWeightEstimator,
                      training_images_path: str,
                      annotations_file: str,
                      epochs: int = 100,
                      batch_size: int = 32):
    """
    Train the weight estimation model with real data
    """
    # Load training data
    with open(annotations_file, 'r') as f:
        annotations = json.load(f)

    # Prepare training data
    features_list = []
    weights_list = []

    for annotation in annotations:
        image_path = f"{training_images_path}/{annotation['image_name']}"
        image = cv2.imread(image_path)

        if image is None:
            continue

        for chicken_data in annotation['chickens']:
            bbox = chicken_data['bbox']
            weight = chicken_data['weight']

            # Extract features
            features = estimator.extract_enhanced_features(image, bbox)
            features_list.append(features)
            weights_list.append(weight)

    # Convert to numpy arrays
    X = np.array(features_list)
    y = np.array(weights_list)

    # Normalize features
    X_scaled = estimator.scaler.fit_transform(X)

    # Create PyTorch dataset
    dataset = torch.utils.data.TensorDataset(
        torch.FloatTensor(X_scaled),
        torch.FloatTensor(y.reshape(-1, 1))
    )

    # Split into train/validation
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])

    # Create data loaders
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

    # Initialize model
    model = WeightRegressionModel(input_size=X.shape[1]).to(estimator.device)

    # Loss and optimizer
    criterion = nn.MSELoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)

    # Training loop
    best_val_loss = float('inf')
    train_losses = []
    val_losses = []

    for epoch in range(epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        for features, weights in train_loader:
            features = features.to(estimator.device)
            weights = weights.to(estimator.device)

            optimizer.zero_grad()
            predictions = model(features)
            loss = criterion(predictions, weights)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()

        # Validation phase
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for features, weights in val_loader:
                features = features.to(estimator.device)
                weights = weights.to(estimator.device)

                predictions = model(features)
                loss = criterion(predictions, weights)
                val_loss += loss.item()

        # Calculate average losses
        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)

        train_losses.append(avg_train_loss)
        val_losses.append(avg_val_loss)

        # Update learning rate
        scheduler.step(avg_val_loss)

        # Save best model
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), 'best_weight_model.pth')

        # Print progress
        if epoch % 10 == 0:
            print(f"Epoch {epoch}/{epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")

    # Load best model
    model.load_state_dict(torch.load('best_weight_model.pth'))
    estimator.weight_regression_model = model

    # Plot training history
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='Training Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training History')
    plt.legend()
    plt.savefig('training_history.png')
    plt.close()

    return model

# Main execution function
def main():
    # Initialize camera calibration (adjust these values based on your camera)
    camera_calib = CameraCalibration(
        focal_length=1000.0,  # Adjust based on your camera
        sensor_width=6.0,     # Typical smartphone sensor
        sensor_height=4.5,
        known_object_width=25.0  # Average adult chicken width in cm
    )

    # Initialize the estimator
    estimator = AdvancedChickenWeightEstimator(
        yolo_model_path="/content/runs/detect/train/weights/best.pt",
        camera_calibration=camera_calib
    )

    # Process video with weight estimation
    video_path = "/content/chickvideo.mp4"
    output_path = "/content/chicken_weight_output.mp4"

    print("Processing video with weight estimation...")
    weight_history = estimator.process_video_with_weights(
        video_path=video_path,
        output_path=output_path,
        display_frame_interval=30,
        save_annotations=True
    )

    # Generate summary statistics
    print("\nGenerating summary statistics...")
    with open('chicken_weight_annotations.json', 'r') as f:
        all_annotations = json.load(f)

    # Calculate statistics
    total_detections = sum(a['detections'] for a in all_annotations)
    avg_weight_per_frame = np.mean([a['average_weight'] for a in all_annotations if a['average_weight'] > 0])

    print(f"\nSummary Statistics:")
    print(f"Total frames processed: {len(all_annotations)}")
    print(f"Total chicken detections: {total_detections}")
    print(f"Average weight per chicken: {avg_weight_per_frame:.2f} kg")

    # Create weight distribution plot
    all_weights = []
    for annotation in all_annotations:
        all_weights.extend(annotation['weights'])

    if all_weights:
        plt.figure(figsize=(10, 6))
        plt.hist(all_weights, bins=50, edgecolor='black')
        plt.xlabel('Weight (kg)')
        plt.ylabel('Frequency')
        plt.title('Chicken Weight Distribution')
        plt.savefig('weight_distribution.png')
        plt.close()

    print("\nProcessing complete! Files saved:")
    print("\nProcessing complete! Files saved:")
    print(f"- Output video: {output_path}")
    print("- Annotations: chicken_weight_annotations.json")
    print("- Weight distribution: weight_distribution.png")

    return estimator

# Additional utility functions for real-world deployment

def calibrate_camera(reference_object_path: str, known_width_cm: float) -> CameraCalibration:
    """
    Calibrate camera using a reference object of known size
    """
    image = cv2.imread(reference_object_path)

    # Detect reference object (could be a marker or known chicken)
    # For simplicity, let's assume manual selection
    print("Please select the reference object in the image...")

    # Simple manual ROI selection
    roi = cv2.selectROI("Select Reference Object", image, fromCenter=False, showCrosshair=True)
    cv2.destroyAllWindows()

    x, y, w, h = roi
    pixel_width = w

    # Calculate pixels per cm
    pixels_per_cm = pixel_width / known_width_cm

    # Estimate focal length (simplified)
    # Assuming standard sensor size
    sensor_width_mm = 6.0
    image_width = image.shape[1]

    # Focal length in pixels
    focal_length_pixels = (pixels_per_cm * known_width_cm * image_width) / sensor_width_mm

    calibration = CameraCalibration(
        focal_length=focal_length_pixels,
        sensor_width=sensor_width_mm,
        sensor_height=4.5,
        known_object_width=known_width_cm
    )

    # Save calibration
    with open('camera_calibration.json', 'w') as f:
        json.dump({
            'focal_length': calibration.focal_length,
            'sensor_width': calibration.sensor_width,
            'sensor_height': calibration.sensor_height,
            'known_object_width': calibration.known_object_width,
            'pixels_per_cm': pixels_per_cm
        }, f, indent=2)

    print(f"Camera calibrated! Pixels per cm: {pixels_per_cm:.2f}")
    return calibration

def batch_process_images(estimator: AdvancedChickenWeightEstimator,
                        image_folder: str,
                        output_folder: str):
    """
    Process multiple images for weight estimation
    """
    import glob
    import os

    os.makedirs(output_folder, exist_ok=True)

    image_paths = glob.glob(f"{image_folder}/*.jpg") + glob.glob(f"{image_folder}/*.png")

    results = []

    for image_path in image_paths:
        print(f"Processing: {image_path}")

        # Load image
        image = cv2.imread(image_path)
        if image is None:
            continue

        # Detect chickens
        yolo_results = estimator.yolo_model(image_path)

        annotated_image = image.copy()
        image_weights = []

        if yolo_results[0].boxes is not None:
            for idx, box in enumerate(yolo_results[0].boxes.data.tolist()):
                x1, y1, x2, y2, conf, cls = box

                if int(cls) == 0 and conf > 0.5:
                    bbox = [x1, y1, x2, y2]

                    # Extract features and estimate weight
                    features = estimator.extract_enhanced_features(image, bbox)
                    weight_kg = estimator.ensemble_weight_prediction(image, bbox, features)

                    image_weights.append(weight_kg)

                    # Annotate
                    estimator._annotate_frame(annotated_image, bbox, weight_kg, idx, conf)

        # Save annotated image
        output_path = os.path.join(output_folder, os.path.basename(image_path))
        cv2.imwrite(output_path, annotated_image)

        # Store results
        results.append({
            'image': os.path.basename(image_path),
            'detections': len(image_weights),
            'weights': image_weights,
            'total_weight': sum(image_weights),
            'average_weight': np.mean(image_weights) if image_weights else 0
        })

    # Save batch results
    with open(os.path.join(output_folder, 'batch_results.json'), 'w') as f:
        json.dump(results, f, indent=2)

    print(f"\nBatch processing complete! Processed {len(image_paths)} images.")
    return results

def real_time_weight_monitoring(estimator: AdvancedChickenWeightEstimator,
                               camera_index: int = 0,
                               display_window: bool = True):
    """
    Real-time weight monitoring from camera feed
    """
    cap = cv2.VideoCapture(camera_index)

    # Set camera properties for better quality
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)
    cap.set(cv2.CAP_PROP_FPS, 30)

    weight_history = []
    frame_count = 0

    print("Starting real-time weight monitoring. Press 'q' to quit, 's' to save snapshot.")

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        frame_count += 1

        # Process every 5th frame for performance
        if frame_count % 5 == 0:
            # Detect chickens
            results = estimator.yolo_model(frame, conf=0.5)

            current_weights = []
            annotated_frame = frame.copy()

            if results[0].boxes is not None:
                for idx, box in enumerate(results[0].boxes.data.tolist()):
                    x1, y1, x2, y2, conf, cls = box

                    if int(cls) == 0 and conf > 0.5:
                        bbox = [x1, y1, x2, y2]

                        # Estimate weight
                        features = estimator.extract_enhanced_features(frame, bbox)
                        weight_kg = estimator.ensemble_weight_prediction(frame, bbox, features)

                        current_weights.append(weight_kg)

                        # Annotate frame
                        estimator._annotate_frame(annotated_frame, bbox, weight_kg, idx, conf)

            # Add monitoring info
            total_weight = sum(current_weights)
            avg_weight = np.mean(current_weights) if current_weights else 0

            info_text = [
                f"Chickens: {len(current_weights)}",
                f"Total Weight: {total_weight:.2f} kg",
                f"Avg Weight: {avg_weight:.2f} kg"
            ]

            y_offset = 30
            for text in info_text:
                cv2.putText(annotated_frame, text, (10, y_offset),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                y_offset += 30

            # Update history
            weight_history.append({
                'timestamp': time.time(),
                'count': len(current_weights),
                'weights': current_weights,
                'total': total_weight,
                'average': avg_weight
            })

            # Keep only last 1000 records
            if len(weight_history) > 1000:
                weight_history.pop(0)

            # Display
            if display_window:
                cv2.imshow('Chicken Weight Monitoring', annotated_frame)

            # Handle key press
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break
            elif key == ord('s'):
                # Save snapshot
                timestamp = time.strftime("%Y%m%d_%H%M%S")
                cv2.imwrite(f"snapshot_{timestamp}.jpg", annotated_frame)
                print(f"Snapshot saved: snapshot_{timestamp}.jpg")

    # Cleanup
    cap.release()
    cv2.destroyAllWindows()

    # Save monitoring history
    with open('monitoring_history.json', 'w') as f:
        json.dump(weight_history, f, indent=2)

    print("\nMonitoring stopped. History saved to monitoring_history.json")
    return weight_history

# Data augmentation for training
class ChickenDataAugmentation:
    """
    Data augmentation specifically for chicken weight estimation
    """
    @staticmethod
    def augment_image_and_bbox(image: np.ndarray, bbox: List[float], weight: float):
        """
        Apply augmentations while maintaining weight relationships
        """
        augmented_samples = []

        x1, y1, x2, y2 = bbox
        h, w = image.shape[:2]

        # Original sample
        augmented_samples.append({
            'image': image.copy(),
            'bbox': bbox.copy(),
            'weight': weight
        })

        # 1. Brightness variations (doesn't affect weight)
        for brightness_factor in [0.7, 1.3]:
            bright_image = cv2.convertScaleAbs(image, alpha=brightness_factor, beta=0)
            augmented_samples.append({
                'image': bright_image,
                'bbox': bbox.copy(),
                'weight': weight
            })

        # 2. Slight rotations (weight unchanged)
        for angle in [-10, 10]:
            center = (w // 2, h // 2)
            M = cv2.getRotationMatrix2D(center, angle, 1.0)
            rotated = cv2.warpAffine(image, M, (w, h))

            # Transform bbox
            pts = np.array([[x1, y1], [x2, y1], [x2, y2], [x1, y2]], dtype=np.float32)
            pts = np.concatenate([pts, np.ones((4, 1))], axis=1)
            transformed_pts = (M @ pts.T).T

            new_bbox = [
                np.min(transformed_pts[:, 0]),
                np.min(transformed_pts[:, 1]),
                np.max(transformed_pts[:, 0]),
                np.max(transformed_pts[:, 1])
            ]

            augmented_samples.append({
                'image': rotated,
                'bbox': new_bbox,
                'weight': weight
            })

        # 3. Horizontal flip (common in chicken monitoring)
        flipped = cv2.flip(image, 1)
        flipped_bbox = [w - x2, y1, w - x1, y2]
        augmented_samples.append({
            'image': flipped,
            'bbox': flipped_bbox,
            'weight': weight
        })

        # 4. Slight scaling (affects apparent size but not actual weight)
        for scale in [0.9, 1.1]:
            scaled_w = int(w * scale)
            scaled_h = int(h * scale)
            scaled = cv2.resize(image, (scaled_w, scaled_h))

            # Adjust bbox
            scaled_bbox = [coord * scale for coord in bbox]

            # Crop/pad to original size
            if scale > 1:
                # Crop
                start_x = (scaled_w - w) // 2
                start_y = (scaled_h - h) // 2
                final_image = scaled[start_y:start_y+h, start_x:start_x+w]
                final_bbox = [coord - start_x if i % 2 == 0 else coord - start_y
                             for i, coord in enumerate(scaled_bbox)]
            else:
                # Pad
                pad_x = (w - scaled_w) // 2
                pad_y = (h - scaled_h) // 2
                final_image = cv2.copyMakeBorder(scaled, pad_y, pad_y, pad_x, pad_x,
                                               cv2.BORDER_CONSTANT, value=[0, 0, 0])
                final_bbox = [coord + pad_x if i % 2 == 0 else coord + pad_y
                             for i, coord in enumerate(scaled_bbox)]

            augmented_samples.append({
                'image': final_image,
                'bbox': final_bbox,
                'weight': weight
            })

        return augmented_samples

# Run the complete pipeline
if __name__ == "__main__":
    # Step 1: Camera calibration (if needed)
    #calibration = calibrate_camera("/content/reference_image.jpg", known_width_cm=30.0)

    # Step 2: Initialize estimator with calibration
    estimator = main()

    # Step 3: Batch process images (if available)
    batch_results = batch_process_images(estimator, "/content/train/images", "/content/output_images")

    # Step 4: Real-time monitoring (if camera available)
    # monitoring_history = real_time_weight_monitoring(estimator, camera_index=0)

    print("\nChicken weight estimation system ready!")
    print("Features:")
    print("- Distance-aware weight estimation")
    print("- Age category classification")
    print("- Multiple estimation methods (ensemble)")
    print("- Temporal smoothing for video")
    print("- Real-time monitoring capability")
    print("- Batch processing support")